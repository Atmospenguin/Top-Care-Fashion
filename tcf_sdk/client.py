import base64
import json
import os
import re
import time
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import requests
from bs4 import BeautifulSoup


# ===========================
# è¾…åŠ©ï¼šæšä¸¾æ˜ å°„ & Tag é€»è¾‘
# ===========================

CONDITION_MAP = {
    "brand new": "Brand New",
    "new": "Brand New",
    "like new": "Like New",
    "good": "Good",
    "fair": "Fair",
    "poor": "Poor",
}

GENDER_MAP = {
    "men": "Men",
    "male": "Men",
    "women": "Women",
    "female": "Women",
    "unisex": "Unisex",
    "uni": "Unisex",
    "all": "Unisex",
}

VALID_CATEGORIES = {"Accessories", "Bottoms", "Footwear", "Outerwear", "Tops"}

SPECIAL_BRANDS: Dict[str, List[str]] = {
    "Vivienne Westwood": ["designer", "luxury", "vintage"],
    "Chanel": ["designer", "luxury", "premium"],
    "Gucci": ["designer", "luxury", "premium"],
    "Prada": ["designer", "luxury"],
    "Louis Vuitton": ["designer", "luxury", "premium"],
}


def normalize_tags(tags: Optional[List[str]]) -> List[str]:
    """è§„èŒƒåŒ– tagsï¼šå»é‡ã€å»ç©ºæ ¼ã€å°å†™ã€‚"""
    if not tags:
        return []
    norm: List[str] = []
    for t in tags:
        if not t:
            continue
        v = t.strip().lower()
        if v and v not in norm:
            norm.append(v)
    return norm


def add_special_brand_tags(brand: Optional[str], existing_tags: Optional[List[str]]) -> List[str]:
    """å¯¹ç‰¹æ®Šå“ç‰Œè‡ªåŠ¨é™„åŠ  designer/luxury ç­‰æ ‡ç­¾ã€‚"""
    tags = normalize_tags(existing_tags)
    if not brand:
        return tags
    extra = SPECIAL_BRANDS.get(brand, [])
    for t in extra:
        v = t.lower().strip()
        if v and v not in tags:
            tags.append(v)
    return tags


def guess_category_from_text(text: str) -> str:
    """ç®€å• heuristicï¼šæ ¹æ® Farfetch é¡µé¢æ–‡å­—çŒœæµ‹ ListingCategoryã€‚"""
    t = text.lower()

    if any(k in t for k in ["jeans", "trousers", "pants", "shorts", "skirt"]):
        return "Bottoms"
    if any(k in t for k in ["sneakers", "boots", "sandals", "pumps", "heels", "loafers"]):
        return "Footwear"
    if any(
        k in t
        for k in [
            "jacket",
            "coat",
            "bomber",
            "cardigan",
            "cape",
            "blazer",
            "parka",
            "puffer",
        ]
    ):
        return "Outerwear"
    if any(k in t for k in ["bag", "belt", "wallet", "scarf", "hat", "cap", "accessories"]):
        return "Accessories"
    return "Tops"


def parse_price_from_html(html: str) -> Optional[float]:
    """ä» HTML é‡Œç²—ç•¥æŠ“ä¸€ä¸ªä»·æ ¼ï¼ˆÂ¥ / $ / â‚¬ / Â£ï¼‰ï¼Œåªä¿ç•™æ•°å­—+å°æ•°ç‚¹ã€‚"""
    m = re.search(r"([$â‚¬Â£Â¥]\s*[\d,]+(?:\.\d+)?)", html)
    if not m:
        return None
    raw = m.group(1)
    numeric = re.sub(r"[^\d.]", "", raw)
    try:
        return float(numeric)
    except ValueError:
        return None


def extract_text_list_after_heading(soup: BeautifulSoup, heading_text: str) -> List[str]:
    """åœ¨"Highlights""Composition"è¿™ç§ heading åé¢æŠ“ bullet listã€‚"""
    results: List[str] = []
    heading = None
    for node in soup.find_all(text=True):
        if isinstance(node, str) and heading_text.lower() in node.strip().lower():
            heading = node.parent
            break

    if not heading:
        return results

    ul = heading.find_next("ul")
    if ul:
        for li in ul.find_all("li"):
            t = li.get_text(strip=True)
            if t:
                results.append(t)
    else:
        nxt = heading
        for _ in range(10):
            nxt = nxt.find_next_sibling()
            if not nxt:
                break
            t = nxt.get_text(strip=True)
            if t:
                results.append(t)

    return results


# ===========================
# TCFClientï¼šSDK ä¸»ä½“
# ===========================

DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept": (
        "text/html,application/xhtml+xml,application/xml;q=0.9,"
        "image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
    ),
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "DNT": "1",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Cache-Control": "max-age=0",
}


@dataclass
class TCFClient:
    base_url: str
    token: str = ""
    cookie: str = ""  # å¯é€‰ï¼šCookie å­—ç¬¦ä¸²ç”¨äºè®¤è¯
    timeout: int = 30
    
    def __post_init__(self):
        """åˆå§‹åŒ–åè®¾ç½® cookie"""
        if self.cookie:
            self.set_cookie(self.cookie)

    # ---------- åŸºç¡€ HTTP å°è£… ----------

    @property
    def _auth_headers(self) -> Dict[str, str]:
        """æ„å»ºè®¤è¯ headersï¼Œæ”¯æŒ Bearer token æˆ– Cookie"""
        headers = {
            "Content-Type": "application/json",
        }
        
        # å¦‚æœæä¾›äº† tokenï¼Œä½¿ç”¨ Bearer token
        if self.token:
            headers["Authorization"] = f"Bearer {self.token}"
        
        return headers
    
    def set_cookie(self, cookie_string: str):
        """è®¾ç½® Cookie ç”¨äºè®¤è¯ï¼ˆæ›¿ä»£æˆ–è¡¥å…… Bearer tokenï¼‰"""
        self._cookie = cookie_string
    
    @property
    def _cookie(self) -> Optional[str]:
        """è·å– Cookie å­—ç¬¦ä¸²"""
        return getattr(self, '_cookie_value', None)
    
    @_cookie.setter
    def _cookie(self, value: str):
        """è®¾ç½® Cookie å­—ç¬¦ä¸²"""
        self._cookie_value = value

    def _post(self, path: str, json_body: Dict[str, Any]) -> Tuple[int, Any]:
        url = self.base_url.rstrip("/") + path
        headers = self._auth_headers.copy()
        
        # å¦‚æœè®¾ç½®äº† Cookieï¼Œæ·»åŠ åˆ° headers
        if self._cookie:
            headers["Cookie"] = self._cookie
        
        resp = requests.post(url, headers=headers, json=json_body, timeout=self.timeout)
        try:
            data = resp.json()
        except Exception:
            data = resp.text
        return resp.status_code, data

    # ---------- ä¸Šä¼ å›¾ç‰‡ ----------

    def upload_image_file(self, image_path: str) -> Optional[str]:
        """è¯»å–æœ¬åœ°æ–‡ä»¶ â†’ base64 â†’ è°ƒç”¨ /api/listings/upload-image â†’ è¿”å› imageUrlã€‚"""
        if not os.path.exists(image_path):
            print(f"âŒ å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
            return None

        with open(image_path, "rb") as f:
            image_data = f.read()
        b64 = base64.b64encode(image_data).decode("utf-8")
        filename = os.path.basename(image_path)

        payload = {"imageData": b64, "fileName": filename}

        status, data = self._post("/api/listings/upload-image", payload)
        if status == 200 and isinstance(data, dict) and data.get("imageUrl"):
            url = data["imageUrl"]
            print(f"âœ… å›¾ç‰‡ä¸Šä¼ æˆåŠŸ: {url}")
            return url

        print(f"âŒ å›¾ç‰‡ä¸Šä¼ å¤±è´¥: HTTP {status}, å“åº”: {data}")
        return None

    # ---------- åˆ›å»º Listing ----------

    def create_listing(self, listing_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        æŒ‰ç…§ä½ ç»™çš„ CreateListingRequest schema è°ƒç”¨ /api/listings/createã€‚
        è‡ªåŠ¨å¤„ç†é”™è¯¯ä¿¡æ¯ã€‚
        """
        status, data = self._post("/api/listings/create", listing_data)

        if status == 200 and isinstance(data, dict) and data.get("success") and data.get("data"):
            print(f"âœ… Listing åˆ›å»ºæˆåŠŸ: {data['data'].get('id')} | {data['data'].get('title')}")
            return data["data"]

        # é”™è¯¯å¤„ç†
        if status == 401:
            print("âŒ 401 Unauthorized: Token æ— æ•ˆæˆ–å·²è¿‡æœŸ")
        elif status == 403:
            print(f"âŒ 403 Forbidden: {data}")
        elif status == 400:
            print(f"âŒ 400 Bad Request: {data}")
        else:
            print(f"âŒ åˆ›å»ºå¤±è´¥ HTTP {status}: {data}")
        return None

    # ===========================
    # Farfetch è§£æ + æ˜ å°„åˆ° Listing Schema
    # ===========================

    def scrape_farfetch_product(self, url: str) -> Optional[Dict[str, Any]]:
        """
        ä» Farfetch å•†å“é¡µé¢æŠ“å–ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆ CreateListingRequest çš„ dictã€‚
        """
        print(f"ğŸŒ æŠ“å– Farfetch å•†å“: {url}")
        
        # å¼ºåŒ–æµè§ˆå™¨ä¼ªè£… headers
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
            "Accept": (
                "text/html,application/xhtml+xml,application/xml;q=0.9,"
                "image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7"
            ),
            "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Cache-Control": "max-age=0",
            "Referer": "https://www.farfetch.com/",
        }
        
        # åˆ›å»º session ä»¥ä¿æŒ cookies å’Œè¿æ¥
        session = requests.Session()
        session.headers.update(headers)
        
        # é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3æ¬¡ï¼‰
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # å…ˆè®¿é—®ä¸»é¡µè·å– cookiesï¼ˆæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸ºï¼‰
                if attempt == 0:
                    print("   ğŸ“ è®¿é—®ä¸»é¡µè·å– cookies...")
                    session.get("https://www.farfetch.com", timeout=self.timeout)
                    time.sleep(1 + attempt * 0.5)  # é€’å¢å»¶è¿Ÿ
                
                # è®¿é—®ç›®æ ‡é¡µé¢
                print(f"   ğŸ”„ å°è¯•è®¿é—®å•†å“é¡µé¢ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)...")
                resp = session.get(url, timeout=self.timeout, allow_redirects=True)
                
                if resp.status_code == 200:
                    print("   âœ… è·å–é¡µé¢æˆåŠŸ")
                    break
                elif resp.status_code == 403:
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 2
                        print(f"   âš ï¸ HTTP 403ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"   âŒ HTTP 403 Forbidden - Farfetch æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è¯·æ±‚")
                        print(f"      å»ºè®®ï¼š1) æ‰‹åŠ¨åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€é¡µé¢è·å–ä¿¡æ¯")
                        print(f"            2) æˆ–è€…ä½¿ç”¨æµè§ˆå™¨æ‰©å±•å¯¼å‡ºæ•°æ®")
                        print(f"            3) æˆ–è€…ç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•")
                        return None
                else:
                    print(f"   âŒ HTTP {resp.status_code} è·å–é¡µé¢å¤±è´¥")
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        continue
                    return None
                    
            except requests.RequestException as e:
                print(f"   âŒ è¯·æ±‚å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                return None
        else:
            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
            return None

        html = resp.text
        soup = BeautifulSoup(html, "html.parser")

        # ----- æ ‡é¢˜ & å“ç‰Œ -----
        og_title_tag = soup.find("meta", property="og:title")
        if og_title_tag and og_title_tag.get("content"):
            og_title = og_title_tag["content"].strip()
        else:
            # å°è¯•ä» title æ ‡ç­¾è·å–
            title_tag = soup.find("title")
            if title_tag and title_tag.string:
                og_title = title_tag.string.strip()
            else:
                # å°è¯•ä» h1 æˆ–å…¶ä»–æ ‡é¢˜æ ‡ç­¾è·å–
                h1_tag = soup.find("h1")
                if h1_tag:
                    og_title = h1_tag.get_text(strip=True)
                else:
                    og_title = "Untitled"

        # æ¸…ç†æ ‡é¢˜ï¼ˆç§»é™¤ "| Farfetch" ç­‰åç¼€ï¼‰
        main_part = og_title.split("|")[0].strip()
        main_part = main_part.split("- Farfetch")[0].strip()
        
        words = main_part.split()
        if len(words) > 1:
            brand = words[0]
            product_name = " ".join(words[1:])
        else:
            brand = main_part
            product_name = main_part
        
        print(f"   ğŸ‘— æå–å“ç‰Œ: {brand}")
        print(f"   ğŸ“ æå–æ ‡é¢˜: {product_name}")

        # ----- ä»·æ ¼ -----
        price = None
        # æ–¹æ³•1: ä» meta æ ‡ç­¾è·å–
        meta_price = soup.find("meta", property="product:price:amount")
        if meta_price and meta_price.get("content"):
            try:
                price = float(meta_price["content"])
            except ValueError:
                pass
        
        # æ–¹æ³•2: ä» JSON-LD ç»“æ„åŒ–æ•°æ®è·å–
        if price is None:
            json_ld_scripts = soup.find_all("script", type="application/ld+json")
            for script in json_ld_scripts:
                try:
                    data = json.loads(script.string)
                    if isinstance(data, dict) and "offers" in data:
                        offers = data["offers"]
                        if isinstance(offers, dict) and "price" in offers:
                            price = float(offers["price"])
                            break
                        elif isinstance(offers, list) and len(offers) > 0:
                            if "price" in offers[0]:
                                price = float(offers[0]["price"])
                                break
                except (json.JSONDecodeError, ValueError, KeyError):
                    continue
        
        # æ–¹æ³•3: ä» HTML æ–‡æœ¬ä¸­è§£æ
        if price is None:
            price = parse_price_from_html(html)
        
        if price is None or price <= 0:
            print("   âš ï¸ æœªèƒ½è§£ææœ‰æ•ˆä»·æ ¼ï¼Œè·³è¿‡è¯¥å•†å“")
            return None
        
        print(f"   ğŸ’° æå–ä»·æ ¼: ${price:.2f}")

        # ----- å›¾ç‰‡ -----
        image_urls: List[str] = []
        # æ–¹æ³•1: ä» og:image meta æ ‡ç­¾è·å–
        for meta_img in soup.find_all("meta", property="og:image"):
            src = meta_img.get("content")
            if src and src not in image_urls:
                # ç¡®ä¿æ˜¯å®Œæ•´ URL
                if src.startswith("//"):
                    src = "https:" + src
                elif src.startswith("/"):
                    src = "https://www.farfetch.com" + src
                image_urls.append(src)
        
        # æ–¹æ³•2: ä»å›¾ç‰‡æ ‡ç­¾è·å–ï¼ˆä¼˜å…ˆ farfetch-contents CDNï¼‰
        if not image_urls:
            for img in soup.find_all("img"):
                src = img.get("src") or img.get("data-src") or ""
                if src and ("farfetch-contents" in src or "farfetch" in src.lower()):
                    if src.startswith("//"):
                        src = "https:" + src
                    elif src.startswith("/"):
                        src = "https://www.farfetch.com" + src
                    if src not in image_urls:
                        image_urls.append(src)
        
        print(f"   ğŸ–¼ æå–å›¾ç‰‡: {len(image_urls)} å¼ ")

        # ----- æè¿° -----
        desc_parts: List[str] = []
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc and meta_desc.get("content"):
            desc_parts.append(meta_desc["content"].strip())

        highlights = extract_text_list_after_heading(soup, "Highlights")
        if highlights:
            desc_parts.append("Highlights: " + "; ".join(highlights))

        composition = extract_text_list_after_heading(soup, "Composition")
        if composition:
            desc_parts.append("Composition: " + "; ".join(composition))

        description = "\n".join(desc_parts) or f"{brand} {product_name} from Farfetch."

        # ----- Category -----
        category = guess_category_from_text(html)
        if category not in VALID_CATEGORIES:
            category = "Tops"

        # ----- Condition / Gender / Tags -----
        # å¯¹äº Farfetch ä¸Šæ¥çš„æ–°å“ wishlistï¼Œä¸€èˆ¬å¯ä»¥è®¤ä¸ºæ¥è¿‘å…¨æ–°
        condition_str = "Like New"
        # ä½ çš„ URL éƒ½æ˜¯ /shopping/women/ï¼Œè¿™é‡Œç›´æ¥ç”¨ Women
        gender_str = "Women"

        base_tags = [brand, *product_name.split()]
        tags = normalize_tags(base_tags)
        tags = add_special_brand_tags(brand, tags)

        material_str = "; ".join(composition) if composition else None

        listing_data: Dict[str, Any] = {
            # å¿…éœ€å­—æ®µ
            "title": f"{brand} {product_name}",
            "description": description,
            "price": price,
            "category": category,
            "shippingOption": "Standard",

            # å¯é€‰å­—æ®µ
            "brand": brand,
            "size": None,  # å¦‚éœ€ä»é¡µé¢æŠ“å°ºå¯¸å¯ä»¥å†æ‰©å±•
            "condition": condition_str,
            "material": material_str,
            "tags": tags,
            "gender": gender_str,
            "images": image_urls,
            "shippingFee": None,
            "location": None,
            "quantity": 1,
            "listed": True,
            "sold": False,
        }

        print(
            f"   ğŸ§¾ è§£æå®Œæˆ: {listing_data['title'][:50]}... | "
            f"å“ç‰Œ={brand} | ä»·æ ¼=${price:.2f} | åˆ†ç±»={category}"
        )
        return listing_data

    # ---------- é«˜å±‚å°è£…ï¼šå•ä¸ª URL ----------

    def create_listing_from_farfetch_url(self, url: str) -> Optional[Dict[str, Any]]:
        product = self.scrape_farfetch_product(url)
        if not product:
            return None
        return self.create_listing(product)

    # ---------- é«˜å±‚å°è£…ï¼šä»æ–‡ä»¶æ‰¹é‡å¤„ç† ----------

    def batch_create_from_farfetch_file(self, file_path: str) -> Tuple[int, List[str]]:
        if not os.path.exists(file_path):
            print(f"âŒ URL æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return 0, []

        urls: List[str] = []
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                u = line.strip()
                if u:
                    urls.append(u)

        if not urls:
            print("âš ï¸ æ–‡ä»¶ä¸­æ²¡æœ‰ä»»ä½• URL")
            return 0, []

        print(f"ğŸ“¦ å…± {len(urls)} ä¸ª Farfetch å•†å“é“¾æ¥å¾…å¤„ç†")

        success = 0
        failed: List[str] = []

        for idx, url in enumerate(urls, 1):
            print("\n" + "-" * 70)
            print(f"[{idx}/{len(urls)}] å¤„ç†: {url}")

            listing = self.create_listing_from_farfetch_url(url)
            if listing:
                success += 1
            else:
                failed.append(url)

            # éšæœºå»¶è¿Ÿ 2-4 ç§’ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
            import random
            delay = random.uniform(2, 4)
            time.sleep(delay)

        return success, failed


